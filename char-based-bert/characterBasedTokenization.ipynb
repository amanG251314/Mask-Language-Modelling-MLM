{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhonRdRoeSqc"
      },
      "outputs": [],
      "source": [
        "# Implementation of Masked Letter Guessing Game."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4T_nBFiZ9gqo",
        "outputId": "0030bd91-f4db-4b0e-a5bc-5f0143dd0f3c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "from transformers import CanineTokenizer, CanineModel, Trainer, TrainingArguments\n",
        "import torch.nn as nn\n",
        "# import datasets\n",
        "\n",
        "# # Define your list of words\n",
        "# words = [\"apple\", \"banana\", \"orange\", \"pineapple\", \"grape\", \"watermelon\"]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "e0hzqGZQ9elA"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "eryiysnA9uY5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/words_250000_train.txt'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import shutil\n",
        "\n",
        "# Source path in Google Drive\n",
        "source_path = '../words_250000_train.txt'\n",
        "\n",
        "# Destination path in Colab environment\n",
        "destination_path = '/words_250000_train.txt'\n",
        "\n",
        "# Copy the file from Google Drive to Colab\n",
        "shutil.copyfile(source_path, destination_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "A9dMAT-5-Mf6"
      },
      "outputs": [],
      "source": [
        "# Load data from the text file\n",
        "data_file =\"/words_250000_train.txt\"\n",
        "with open(data_file, \"r\") as file:\n",
        "    words = [line.strip() for line in file]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "words = [\" \".join(list(word)) for word in words]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vhnrrH72-ZcE"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(227300, 'a a a a a a')"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(words), words[1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VrW5wLlYefWb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "# Tokenize the words on character level\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\",vocab_file=\"vocab.txt\")\n",
        "# tokenizer = CanineTokenizer.from_pretrained('google/canine-s',model_max_length=200) #,model_max_length=512\n",
        "tokenized_words = tokenizer(words, padding=True, truncation=True, return_tensors=\"pt\", is_split_into_words=True )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DesqyghbOqiY",
        "outputId": "45994ddf-89af-4bbf-cac2-ce7f659e41bf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "57"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get vocab size from the tokenizer\n",
        "vocab_size = tokenizer.vocab_size\n",
        "vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "42SvK7Nz9GvZ"
      },
      "outputs": [],
      "source": [
        "# Define a custom dataset\n",
        "class MaskedLetterDataset(Dataset):\n",
        "    def __init__(self, tokenized_words, tokenizer):\n",
        "        self.tokenized_words = tokenized_words\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokenized_words[\"input_ids\"])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_ids = self.tokenized_words[\"input_ids\"][idx].clone()\n",
        "        label_ids = input_ids.clone()\n",
        "\n",
        "        # Randomly choose a position to mask\n",
        "        mask_index = torch.randint(1, input_ids.shape[0], (1,)).item()\n",
        "        # Mask the chosen position\n",
        "        input_ids[mask_index] = tokenizer.mask_token_id\n",
        "\n",
        "        return input_ids, label_ids\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTy1-cWU9IOr",
        "outputId": "a49ea13e-74c7-4952-9ff7-0b40330cf743"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "# Create dataset and dataloader\n",
        "dataset = MaskedLetterDataset(tokenized_words, tokenizer)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Initialize pre-trained BERT model for MLM\n",
        "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
        "# model = CanineModel.from_pretrained('google/canine-s')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "FFs9QIIA9LST",
        "outputId": "da5bea6d-915f-4510-a0d2-964120273b18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000, Loss: 8.99145221710205\n",
            "Epoch 2/1000, Loss: 7.057374477386475\n",
            "Epoch 3/1000, Loss: 6.566317081451416\n",
            "Epoch 4/1000, Loss: 6.236056804656982\n",
            "Epoch 5/1000, Loss: 5.9101433753967285\n",
            "Epoch 6/1000, Loss: 5.604120254516602\n",
            "Epoch 7/1000, Loss: 5.311336040496826\n",
            "Epoch 8/1000, Loss: 5.068897724151611\n",
            "Epoch 9/1000, Loss: 4.857880115509033\n",
            "Epoch 10/1000, Loss: 4.667656898498535\n",
            "Epoch 11/1000, Loss: 4.496009826660156\n",
            "Epoch 12/1000, Loss: 4.345120429992676\n",
            "Epoch 13/1000, Loss: 4.211848258972168\n",
            "Epoch 14/1000, Loss: 4.087759017944336\n",
            "Epoch 15/1000, Loss: 3.967772960662842\n",
            "Epoch 16/1000, Loss: 3.858030080795288\n",
            "Epoch 17/1000, Loss: 3.751708507537842\n",
            "Epoch 18/1000, Loss: 3.654512643814087\n",
            "Epoch 19/1000, Loss: 3.561464309692383\n",
            "Epoch 20/1000, Loss: 3.472590446472168\n",
            "Epoch 21/1000, Loss: 3.389397144317627\n",
            "Epoch 22/1000, Loss: 3.3104944229125977\n",
            "Epoch 23/1000, Loss: 3.234757661819458\n",
            "Epoch 24/1000, Loss: 3.162339925765991\n",
            "Epoch 25/1000, Loss: 3.0939877033233643\n",
            "Epoch 26/1000, Loss: 3.0287282466888428\n",
            "Epoch 27/1000, Loss: 2.971067190170288\n",
            "Epoch 28/1000, Loss: 2.909837007522583\n",
            "Epoch 29/1000, Loss: 2.859290599822998\n",
            "Epoch 30/1000, Loss: 2.8071541786193848\n",
            "Epoch 31/1000, Loss: 2.758167266845703\n",
            "Epoch 32/1000, Loss: 2.7172937393188477\n",
            "Epoch 33/1000, Loss: 2.672823429107666\n",
            "Epoch 34/1000, Loss: 2.6327598094940186\n",
            "Epoch 35/1000, Loss: 2.5937178134918213\n",
            "Epoch 36/1000, Loss: 2.554015874862671\n",
            "Epoch 37/1000, Loss: 2.5169193744659424\n",
            "Epoch 38/1000, Loss: 2.474623680114746\n",
            "Epoch 39/1000, Loss: 2.439729928970337\n",
            "Epoch 40/1000, Loss: 2.388206720352173\n",
            "Epoch 41/1000, Loss: 2.332880735397339\n",
            "Epoch 42/1000, Loss: 2.2973058223724365\n",
            "Epoch 43/1000, Loss: 2.3070948123931885\n",
            "Epoch 44/1000, Loss: 2.1726341247558594\n",
            "Epoch 45/1000, Loss: 2.110001802444458\n",
            "Epoch 46/1000, Loss: 2.061889410018921\n",
            "Epoch 47/1000, Loss: 1.9842113256454468\n",
            "Epoch 48/1000, Loss: 1.9335947036743164\n",
            "Epoch 49/1000, Loss: 1.8894191980361938\n",
            "Epoch 50/1000, Loss: 1.8447134494781494\n",
            "Epoch 51/1000, Loss: 1.7907257080078125\n",
            "Epoch 52/1000, Loss: 1.7481353282928467\n",
            "Epoch 53/1000, Loss: 1.7066433429718018\n",
            "Epoch 54/1000, Loss: 1.6466736793518066\n",
            "Epoch 55/1000, Loss: 1.5900044441223145\n",
            "Epoch 56/1000, Loss: 1.5718055963516235\n",
            "Epoch 57/1000, Loss: 1.5767688751220703\n",
            "Epoch 58/1000, Loss: 1.4417775869369507\n",
            "Epoch 59/1000, Loss: 1.4263863563537598\n",
            "Epoch 60/1000, Loss: 1.3411768674850464\n",
            "Epoch 61/1000, Loss: 1.2810543775558472\n",
            "Epoch 62/1000, Loss: 1.2156466245651245\n",
            "Epoch 63/1000, Loss: 1.1511980295181274\n",
            "Epoch 64/1000, Loss: 1.1502985954284668\n",
            "Epoch 65/1000, Loss: 1.0801959037780762\n",
            "Epoch 66/1000, Loss: 1.0137932300567627\n",
            "Epoch 67/1000, Loss: 0.9309152364730835\n",
            "Epoch 68/1000, Loss: 0.8956311941146851\n",
            "Epoch 69/1000, Loss: 0.8300004005432129\n",
            "Epoch 70/1000, Loss: 0.7880911827087402\n",
            "Epoch 71/1000, Loss: 0.7377849221229553\n",
            "Epoch 72/1000, Loss: 0.689649760723114\n",
            "Epoch 73/1000, Loss: 0.6359636187553406\n",
            "Epoch 74/1000, Loss: 0.60651034116745\n",
            "Epoch 75/1000, Loss: 0.5607476234436035\n",
            "Epoch 76/1000, Loss: 0.523802638053894\n",
            "Epoch 77/1000, Loss: 0.4979371726512909\n",
            "Epoch 78/1000, Loss: 0.46459293365478516\n",
            "Epoch 79/1000, Loss: 0.4315035939216614\n",
            "Epoch 80/1000, Loss: 0.4009755551815033\n",
            "Epoch 81/1000, Loss: 0.37531840801239014\n",
            "Epoch 82/1000, Loss: 0.3504263758659363\n",
            "Epoch 83/1000, Loss: 0.32151398062705994\n",
            "Epoch 84/1000, Loss: 0.3085363209247589\n",
            "Epoch 85/1000, Loss: 0.287026047706604\n",
            "Epoch 86/1000, Loss: 0.267521470785141\n",
            "Epoch 87/1000, Loss: 0.2493474930524826\n",
            "Epoch 88/1000, Loss: 0.22867079079151154\n",
            "Epoch 89/1000, Loss: 0.22447296977043152\n",
            "Epoch 90/1000, Loss: 0.20265863835811615\n",
            "Epoch 91/1000, Loss: 0.19985903799533844\n",
            "Epoch 92/1000, Loss: 0.18938997387886047\n",
            "Epoch 93/1000, Loss: 0.17508313059806824\n",
            "Epoch 94/1000, Loss: 0.16016682982444763\n",
            "Epoch 95/1000, Loss: 6.431515693664551\n",
            "Epoch 96/1000, Loss: 0.3098744750022888\n",
            "Epoch 97/1000, Loss: 0.5319317579269409\n",
            "Epoch 98/1000, Loss: 0.5399006009101868\n",
            "Epoch 99/1000, Loss: 0.5684942603111267\n",
            "Epoch 100/1000, Loss: 0.47289136052131653\n",
            "Epoch 101/1000, Loss: 0.4462689757347107\n",
            "Epoch 102/1000, Loss: 0.3713599741458893\n",
            "Epoch 103/1000, Loss: 0.37222737073898315\n",
            "Epoch 104/1000, Loss: 0.3289830684661865\n",
            "Epoch 105/1000, Loss: 0.3019559383392334\n",
            "Epoch 106/1000, Loss: 0.267599880695343\n",
            "Epoch 107/1000, Loss: 0.24741266667842865\n",
            "Epoch 108/1000, Loss: 0.2301892787218094\n",
            "Epoch 109/1000, Loss: 0.21234653890132904\n",
            "Epoch 110/1000, Loss: 0.20074352622032166\n",
            "Epoch 111/1000, Loss: 0.1783338487148285\n",
            "Epoch 112/1000, Loss: 0.1641966551542282\n",
            "Epoch 113/1000, Loss: 0.15584823489189148\n",
            "Epoch 114/1000, Loss: 0.14867359399795532\n",
            "Epoch 115/1000, Loss: 0.13246594369411469\n",
            "Epoch 116/1000, Loss: 0.13043633103370667\n",
            "Epoch 117/1000, Loss: 0.12169358879327774\n",
            "Epoch 118/1000, Loss: 0.10755307227373123\n",
            "Epoch 119/1000, Loss: 0.10967051237821579\n",
            "Epoch 120/1000, Loss: 0.10368421673774719\n",
            "Epoch 121/1000, Loss: 0.09892008453607559\n",
            "Epoch 122/1000, Loss: 0.09765062481164932\n",
            "Epoch 123/1000, Loss: 0.0904272273182869\n",
            "Epoch 124/1000, Loss: 0.09039488434791565\n",
            "Epoch 125/1000, Loss: 0.08359918743371964\n",
            "Epoch 126/1000, Loss: 0.07548391073942184\n",
            "Epoch 127/1000, Loss: 0.07845296710729599\n",
            "Epoch 128/1000, Loss: 0.0700664073228836\n",
            "Epoch 129/1000, Loss: 0.07285574078559875\n",
            "Epoch 130/1000, Loss: 0.07067074626684189\n",
            "Epoch 131/1000, Loss: 0.06713888049125671\n",
            "Epoch 132/1000, Loss: 0.0646234080195427\n",
            "Epoch 133/1000, Loss: 0.06463368982076645\n",
            "Epoch 134/1000, Loss: 0.06113700196146965\n",
            "Epoch 135/1000, Loss: 0.06017693504691124\n",
            "Epoch 136/1000, Loss: 0.059865981340408325\n",
            "Epoch 137/1000, Loss: 0.05670107901096344\n",
            "Epoch 138/1000, Loss: 0.055611543357372284\n",
            "Epoch 139/1000, Loss: 0.05524482578039169\n",
            "Epoch 140/1000, Loss: 0.048336517065763474\n",
            "Epoch 141/1000, Loss: 0.04734065383672714\n",
            "Epoch 142/1000, Loss: 0.051797740161418915\n",
            "Epoch 143/1000, Loss: 0.04665231332182884\n",
            "Epoch 144/1000, Loss: 0.05027191713452339\n",
            "Epoch 145/1000, Loss: 0.04282599687576294\n",
            "Epoch 146/1000, Loss: 0.04699232801795006\n",
            "Epoch 147/1000, Loss: 0.04795132949948311\n",
            "Epoch 148/1000, Loss: 0.046930450946092606\n",
            "Epoch 149/1000, Loss: 0.04410426318645477\n",
            "Epoch 150/1000, Loss: 0.04114370420575142\n",
            "Epoch 151/1000, Loss: 0.04382048174738884\n",
            "Epoch 152/1000, Loss: 0.037703339010477066\n",
            "Epoch 153/1000, Loss: 0.03913969546556473\n",
            "Epoch 154/1000, Loss: 0.04051101952791214\n",
            "Epoch 155/1000, Loss: 0.042288973927497864\n",
            "Epoch 156/1000, Loss: 0.039529506117105484\n",
            "Epoch 157/1000, Loss: 0.042340077459812164\n",
            "Epoch 158/1000, Loss: 0.038542553782463074\n",
            "Epoch 159/1000, Loss: 0.04061292111873627\n",
            "Epoch 160/1000, Loss: 0.03876310586929321\n",
            "Epoch 161/1000, Loss: 0.03318613022565842\n",
            "Epoch 162/1000, Loss: 0.034580498933792114\n",
            "Epoch 163/1000, Loss: 0.0404549278318882\n",
            "Epoch 164/1000, Loss: 0.03445494547486305\n",
            "Epoch 165/1000, Loss: 0.038024045526981354\n",
            "Epoch 166/1000, Loss: 0.03768718242645264\n",
            "Epoch 167/1000, Loss: 0.03700302913784981\n",
            "Epoch 168/1000, Loss: 0.03282642737030983\n",
            "Epoch 169/1000, Loss: 0.03021514229476452\n",
            "Epoch 170/1000, Loss: 0.030176088213920593\n",
            "Epoch 171/1000, Loss: 0.033334728330373764\n",
            "Epoch 172/1000, Loss: 0.029102254658937454\n",
            "Epoch 173/1000, Loss: 0.029136408120393753\n",
            "Epoch 174/1000, Loss: 0.030055636540055275\n",
            "Epoch 175/1000, Loss: 0.028194522485136986\n",
            "Epoch 176/1000, Loss: 0.03363019600510597\n",
            "Epoch 177/1000, Loss: 0.03653109446167946\n",
            "Epoch 178/1000, Loss: 0.034155864268541336\n",
            "Epoch 179/1000, Loss: 0.03322784975171089\n",
            "Epoch 180/1000, Loss: 0.028616122901439667\n",
            "Epoch 181/1000, Loss: 0.026489784941077232\n",
            "Epoch 182/1000, Loss: 0.032628994435071945\n",
            "Epoch 183/1000, Loss: 0.02635517157614231\n",
            "Epoch 184/1000, Loss: 0.03299323841929436\n",
            "Epoch 185/1000, Loss: 0.03276107460260391\n",
            "Epoch 186/1000, Loss: 0.029639096930623055\n",
            "Epoch 187/1000, Loss: 0.03077421896159649\n",
            "Epoch 188/1000, Loss: 0.030643034726381302\n",
            "Epoch 189/1000, Loss: 0.03073015995323658\n",
            "Epoch 190/1000, Loss: 0.028224492445588112\n",
            "Epoch 191/1000, Loss: 0.02808734029531479\n",
            "Epoch 192/1000, Loss: 0.027706334367394447\n",
            "Epoch 193/1000, Loss: 0.02500464767217636\n",
            "Epoch 194/1000, Loss: 0.031030695885419846\n",
            "Epoch 195/1000, Loss: 0.028976498171687126\n",
            "Epoch 196/1000, Loss: 0.02864404022693634\n",
            "Epoch 197/1000, Loss: 0.033973537385463715\n",
            "Epoch 198/1000, Loss: 0.023090768605470657\n",
            "Epoch 199/1000, Loss: 0.026399174705147743\n",
            "Epoch 200/1000, Loss: 0.02400784194469452\n",
            "Epoch 201/1000, Loss: 0.0247334036976099\n",
            "Epoch 202/1000, Loss: 0.02826407179236412\n",
            "Epoch 203/1000, Loss: 0.02658131718635559\n",
            "Epoch 204/1000, Loss: 0.02238510735332966\n",
            "Epoch 205/1000, Loss: 0.022794440388679504\n",
            "Epoch 206/1000, Loss: 0.02686900645494461\n",
            "Epoch 207/1000, Loss: 0.026466764509677887\n",
            "Epoch 208/1000, Loss: 0.028497381135821342\n",
            "Epoch 209/1000, Loss: 0.02375922165811062\n",
            "Epoch 210/1000, Loss: 0.02461363561451435\n",
            "Epoch 211/1000, Loss: 0.026616008952260017\n",
            "Epoch 212/1000, Loss: 0.026310166344046593\n",
            "Epoch 213/1000, Loss: 0.02726701647043228\n",
            "Epoch 214/1000, Loss: 0.021649684756994247\n",
            "Epoch 215/1000, Loss: 0.02314584329724312\n",
            "Epoch 216/1000, Loss: 0.02627752348780632\n",
            "Epoch 217/1000, Loss: 0.025988692417740822\n",
            "Epoch 218/1000, Loss: 0.019467346370220184\n",
            "Epoch 219/1000, Loss: 0.020604202523827553\n",
            "Epoch 220/1000, Loss: 0.020490802824497223\n",
            "Epoch 221/1000, Loss: 0.01905733533203602\n",
            "Epoch 222/1000, Loss: 0.021938130259513855\n",
            "Epoch 223/1000, Loss: 0.018781768158078194\n",
            "Epoch 224/1000, Loss: 0.02283133938908577\n",
            "Epoch 225/1000, Loss: 0.018625767901539803\n",
            "Epoch 226/1000, Loss: 0.020325038582086563\n",
            "Epoch 227/1000, Loss: 0.018367255106568336\n",
            "Epoch 228/1000, Loss: 0.01831335946917534\n",
            "Epoch 229/1000, Loss: 0.021748194471001625\n",
            "Epoch 230/1000, Loss: 0.018076961860060692\n",
            "Epoch 231/1000, Loss: 0.01782263070344925\n",
            "Epoch 232/1000, Loss: 0.026881014928221703\n",
            "Epoch 233/1000, Loss: 0.023673683404922485\n",
            "Epoch 234/1000, Loss: 0.023081835359334946\n",
            "Epoch 235/1000, Loss: 0.01983536407351494\n",
            "Epoch 236/1000, Loss: 0.0217692069709301\n",
            "Epoch 237/1000, Loss: 0.02355561964213848\n",
            "Epoch 238/1000, Loss: 0.024090394377708435\n",
            "Epoch 239/1000, Loss: 0.023996639996767044\n",
            "Epoch 240/1000, Loss: 0.017743322998285294\n",
            "Epoch 241/1000, Loss: 0.024082422256469727\n",
            "Epoch 242/1000, Loss: 0.023263700306415558\n",
            "Epoch 243/1000, Loss: 0.017158176749944687\n",
            "Epoch 244/1000, Loss: 0.01696247048676014\n",
            "Epoch 245/1000, Loss: 0.01910938322544098\n",
            "Epoch 246/1000, Loss: 0.016615096479654312\n",
            "Epoch 247/1000, Loss: 0.01609167456626892\n",
            "Epoch 248/1000, Loss: 0.02301180176436901\n",
            "Epoch 249/1000, Loss: 0.01575431413948536\n",
            "Epoch 250/1000, Loss: 0.0185077041387558\n",
            "Epoch 251/1000, Loss: 0.015567991882562637\n",
            "Epoch 252/1000, Loss: 0.018249424174427986\n",
            "Epoch 253/1000, Loss: 0.015440599992871284\n",
            "Epoch 254/1000, Loss: 0.015297000296413898\n",
            "Epoch 255/1000, Loss: 0.0219698715955019\n",
            "Epoch 256/1000, Loss: 0.02077917382121086\n",
            "Epoch 257/1000, Loss: 0.015005084685981274\n",
            "Epoch 258/1000, Loss: 0.020284634083509445\n",
            "Epoch 259/1000, Loss: 0.014916343614459038\n",
            "Epoch 260/1000, Loss: 0.01734384521842003\n",
            "Epoch 261/1000, Loss: 0.016851114109158516\n",
            "Epoch 262/1000, Loss: 0.02025051973760128\n",
            "Epoch 263/1000, Loss: 0.0205556508153677\n",
            "Epoch 264/1000, Loss: 0.019522037357091904\n",
            "Epoch 265/1000, Loss: 0.01925653964281082\n",
            "Epoch 266/1000, Loss: 0.017840223386883736\n",
            "Epoch 267/1000, Loss: 0.015306894667446613\n",
            "Epoch 268/1000, Loss: 0.017646530643105507\n",
            "Epoch 269/1000, Loss: 0.020279396325349808\n",
            "Epoch 270/1000, Loss: 0.020744411274790764\n",
            "Epoch 271/1000, Loss: 0.020399434491991997\n",
            "Epoch 272/1000, Loss: 0.014643900096416473\n",
            "Epoch 273/1000, Loss: 0.019007163122296333\n",
            "Epoch 274/1000, Loss: 0.013765637762844563\n",
            "Epoch 275/1000, Loss: 0.018862493336200714\n",
            "Epoch 276/1000, Loss: 0.019574271515011787\n",
            "Epoch 277/1000, Loss: 0.019058430567383766\n",
            "Epoch 278/1000, Loss: 0.019302619621157646\n",
            "Epoch 279/1000, Loss: 0.0155266048386693\n",
            "Epoch 280/1000, Loss: 0.013820110820233822\n",
            "Epoch 281/1000, Loss: 0.015696436166763306\n",
            "Epoch 282/1000, Loss: 0.015191244892776012\n",
            "Epoch 283/1000, Loss: 0.012957601808011532\n",
            "Epoch 284/1000, Loss: 0.01875527948141098\n",
            "Epoch 285/1000, Loss: 0.012901518493890762\n",
            "Epoch 286/1000, Loss: 0.02087680622935295\n",
            "Epoch 287/1000, Loss: 0.019203340634703636\n",
            "Epoch 288/1000, Loss: 0.014392213895916939\n",
            "Epoch 289/1000, Loss: 0.017512304708361626\n",
            "Epoch 290/1000, Loss: 0.012416277080774307\n",
            "Epoch 291/1000, Loss: 0.017530256882309914\n",
            "Epoch 292/1000, Loss: 0.02168656326830387\n",
            "Epoch 293/1000, Loss: 0.019511645659804344\n",
            "Epoch 294/1000, Loss: 0.01822006329894066\n",
            "Epoch 295/1000, Loss: 0.013533786870539188\n",
            "Epoch 296/1000, Loss: 0.01690347120165825\n",
            "Epoch 297/1000, Loss: 0.013718885369598866\n",
            "Epoch 298/1000, Loss: 0.013897924683988094\n",
            "Epoch 299/1000, Loss: 0.014386111870408058\n",
            "Epoch 300/1000, Loss: 0.01890973374247551\n",
            "Epoch 301/1000, Loss: 0.017860030755400658\n",
            "Epoch 302/1000, Loss: 0.017829740419983864\n",
            "Epoch 303/1000, Loss: 0.015799032524228096\n",
            "Epoch 304/1000, Loss: 0.012947854585945606\n",
            "Epoch 305/1000, Loss: 0.017340783029794693\n",
            "Epoch 306/1000, Loss: 0.011528071016073227\n",
            "Epoch 307/1000, Loss: 0.014097263105213642\n",
            "Epoch 308/1000, Loss: 0.016924697905778885\n",
            "Epoch 309/1000, Loss: 0.01357568521052599\n",
            "Epoch 310/1000, Loss: 0.016963718459010124\n",
            "Epoch 311/1000, Loss: 0.017346803098917007\n",
            "Epoch 312/1000, Loss: 0.018548686057329178\n",
            "Epoch 313/1000, Loss: 0.016308743506669998\n",
            "Epoch 314/1000, Loss: 0.01738918200135231\n",
            "Epoch 315/1000, Loss: 2.7400033473968506\n",
            "Epoch 316/1000, Loss: 0.01686626672744751\n",
            "Epoch 317/1000, Loss: 0.011695481836795807\n",
            "Epoch 318/1000, Loss: 0.01706668548285961\n",
            "Epoch 319/1000, Loss: 0.01754978485405445\n",
            "Epoch 320/1000, Loss: 0.021128440275788307\n",
            "Epoch 321/1000, Loss: 0.01608954928815365\n",
            "Epoch 322/1000, Loss: 0.017766987904906273\n",
            "Epoch 323/1000, Loss: 0.016601361334323883\n",
            "Epoch 324/1000, Loss: 0.013674687594175339\n",
            "Epoch 325/1000, Loss: 0.013044360093772411\n",
            "Epoch 326/1000, Loss: 0.01798222027719021\n",
            "Epoch 327/1000, Loss: 0.01611846126616001\n",
            "Epoch 328/1000, Loss: 0.010610295459628105\n",
            "Epoch 329/1000, Loss: 0.016156993806362152\n",
            "Epoch 330/1000, Loss: 0.010731819085776806\n",
            "Epoch 331/1000, Loss: 0.01183297485113144\n",
            "Epoch 332/1000, Loss: 0.01534252893179655\n",
            "Epoch 333/1000, Loss: 0.014954261481761932\n",
            "Epoch 334/1000, Loss: 0.014904683455824852\n",
            "Epoch 335/1000, Loss: 0.011836033314466476\n",
            "Epoch 336/1000, Loss: 0.010043801739811897\n",
            "Epoch 337/1000, Loss: 0.010192295536398888\n",
            "Epoch 338/1000, Loss: 0.015376354567706585\n",
            "Epoch 339/1000, Loss: 0.015527090057730675\n",
            "Epoch 340/1000, Loss: 0.013922655954957008\n",
            "Epoch 341/1000, Loss: 0.009831728413701057\n",
            "Epoch 342/1000, Loss: 0.009745311923325062\n",
            "Epoch 343/1000, Loss: 0.014941012486815453\n",
            "Epoch 344/1000, Loss: 0.014794968068599701\n",
            "Epoch 345/1000, Loss: 0.011537588201463223\n",
            "Epoch 346/1000, Loss: 0.015869438648223877\n",
            "Epoch 347/1000, Loss: 0.014264480210840702\n",
            "Epoch 348/1000, Loss: 0.01521217543631792\n",
            "Epoch 349/1000, Loss: 3.282132625579834\n",
            "Epoch 350/1000, Loss: 0.017156291753053665\n",
            "Epoch 351/1000, Loss: 0.012758981436491013\n",
            "Epoch 352/1000, Loss: 0.015142431482672691\n",
            "Epoch 353/1000, Loss: 0.011086130514740944\n",
            "Epoch 354/1000, Loss: 0.017386063933372498\n",
            "Epoch 355/1000, Loss: 0.01677526906132698\n",
            "Epoch 356/1000, Loss: 0.016293125227093697\n",
            "Epoch 357/1000, Loss: 0.012235717847943306\n",
            "Epoch 358/1000, Loss: 0.013564934954047203\n",
            "Epoch 359/1000, Loss: 0.0123622240498662\n",
            "Epoch 360/1000, Loss: 0.01502242125570774\n",
            "Epoch 361/1000, Loss: 0.012930197641253471\n",
            "Epoch 362/1000, Loss: 0.014058238826692104\n",
            "Epoch 363/1000, Loss: 0.012112120166420937\n",
            "Epoch 364/1000, Loss: 0.01234482042491436\n",
            "Epoch 365/1000, Loss: 0.011918488889932632\n",
            "Epoch 366/1000, Loss: 0.01928875409066677\n",
            "Epoch 367/1000, Loss: 0.01395413838326931\n",
            "Epoch 368/1000, Loss: 0.018948359414935112\n",
            "Epoch 369/1000, Loss: 0.022050466388463974\n",
            "Epoch 370/1000, Loss: 0.01569509506225586\n",
            "Epoch 371/1000, Loss: 0.0110464533790946\n",
            "Epoch 372/1000, Loss: 0.010634302161633968\n",
            "Epoch 373/1000, Loss: 0.01395582128316164\n",
            "Epoch 374/1000, Loss: 0.015689322724938393\n",
            "Epoch 375/1000, Loss: 0.010731484740972519\n",
            "Epoch 376/1000, Loss: 0.010249366983771324\n",
            "Epoch 377/1000, Loss: 0.010045244358479977\n",
            "Epoch 378/1000, Loss: 0.014267724938690662\n",
            "Epoch 379/1000, Loss: 0.01004884671419859\n",
            "Epoch 380/1000, Loss: 0.020382968708872795\n",
            "Epoch 381/1000, Loss: 0.0184110626578331\n",
            "Epoch 382/1000, Loss: 0.021055271849036217\n",
            "Epoch 383/1000, Loss: 0.01909547671675682\n",
            "Epoch 384/1000, Loss: 0.013842551037669182\n",
            "Epoch 385/1000, Loss: 0.00953303836286068\n",
            "Epoch 386/1000, Loss: 0.009346939623355865\n",
            "Epoch 387/1000, Loss: 0.010719211772084236\n",
            "Epoch 388/1000, Loss: 0.009441125206649303\n",
            "Epoch 389/1000, Loss: 0.0135883754119277\n",
            "Epoch 390/1000, Loss: 0.01373871136456728\n",
            "Epoch 391/1000, Loss: 0.014095905236899853\n",
            "Epoch 392/1000, Loss: 0.00887707993388176\n",
            "Epoch 393/1000, Loss: 0.013518578372895718\n",
            "Epoch 394/1000, Loss: 0.011248578317463398\n",
            "Epoch 395/1000, Loss: 0.009153996594250202\n",
            "Epoch 396/1000, Loss: 0.01382412388920784\n",
            "Epoch 397/1000, Loss: 0.008599680848419666\n",
            "Epoch 398/1000, Loss: 0.008748989552259445\n",
            "Epoch 399/1000, Loss: 0.01540550496429205\n",
            "Epoch 400/1000, Loss: 0.008485803380608559\n",
            "Epoch 401/1000, Loss: 0.013989284634590149\n",
            "Epoch 402/1000, Loss: 0.008388178423047066\n",
            "Epoch 403/1000, Loss: 0.018204763531684875\n",
            "Epoch 404/1000, Loss: 0.013015207834541798\n",
            "Epoch 405/1000, Loss: 0.01347622461616993\n",
            "Epoch 406/1000, Loss: 0.012098346836864948\n",
            "Epoch 407/1000, Loss: 0.011070583015680313\n",
            "Epoch 408/1000, Loss: 0.00819302536547184\n",
            "Epoch 409/1000, Loss: 0.008309775032103062\n",
            "Epoch 410/1000, Loss: 0.009109266102313995\n",
            "Epoch 411/1000, Loss: 0.008413927629590034\n",
            "Epoch 412/1000, Loss: 0.011528008617460728\n",
            "Epoch 413/1000, Loss: 0.008371035568416119\n",
            "Epoch 414/1000, Loss: 0.01258817408233881\n",
            "Epoch 415/1000, Loss: 0.012133586220443249\n",
            "Epoch 416/1000, Loss: 0.013103972189128399\n",
            "Epoch 417/1000, Loss: 0.007764373905956745\n",
            "Epoch 418/1000, Loss: 0.008074632845818996\n",
            "Epoch 419/1000, Loss: 0.009637664072215557\n",
            "Epoch 420/1000, Loss: 0.013239028863608837\n",
            "Epoch 421/1000, Loss: 0.011442117393016815\n",
            "Epoch 422/1000, Loss: 0.01677294261753559\n",
            "Epoch 423/1000, Loss: 0.007905542850494385\n",
            "Epoch 424/1000, Loss: 0.014135818928480148\n",
            "Epoch 425/1000, Loss: 0.010618203319609165\n",
            "Epoch 426/1000, Loss: 0.010645316913723946\n",
            "Epoch 427/1000, Loss: 0.009940487332642078\n",
            "Epoch 428/1000, Loss: 0.014251776970922947\n",
            "Epoch 429/1000, Loss: 0.00805457029491663\n",
            "Epoch 430/1000, Loss: 0.007583874277770519\n",
            "Epoch 431/1000, Loss: 0.012263050302863121\n",
            "Epoch 432/1000, Loss: 0.009248137474060059\n",
            "Epoch 433/1000, Loss: 0.013601929880678654\n",
            "Epoch 434/1000, Loss: 0.01849607564508915\n",
            "Epoch 435/1000, Loss: 0.009470317512750626\n",
            "Epoch 436/1000, Loss: 0.011367443017661572\n",
            "Epoch 437/1000, Loss: 0.00717299384996295\n",
            "Epoch 438/1000, Loss: 0.00752593670040369\n",
            "Epoch 439/1000, Loss: 0.011765839532017708\n",
            "Epoch 440/1000, Loss: 0.008151276968419552\n",
            "Epoch 441/1000, Loss: 0.010022539645433426\n",
            "Epoch 442/1000, Loss: 0.007014618255198002\n",
            "Epoch 443/1000, Loss: 0.009168142452836037\n",
            "Epoch 444/1000, Loss: 0.008966426365077496\n",
            "Epoch 445/1000, Loss: 0.006897231098264456\n",
            "Epoch 446/1000, Loss: 0.007008549757301807\n",
            "Epoch 447/1000, Loss: 0.011980941519141197\n",
            "Epoch 448/1000, Loss: 0.011726620607078075\n",
            "Epoch 449/1000, Loss: 0.009547440335154533\n",
            "Epoch 450/1000, Loss: 0.006845354568213224\n",
            "Epoch 451/1000, Loss: 0.015438878908753395\n",
            "Epoch 452/1000, Loss: 0.013499745167791843\n",
            "Epoch 453/1000, Loss: 0.010354781523346901\n",
            "Epoch 454/1000, Loss: 0.0069376155734062195\n",
            "Epoch 455/1000, Loss: 0.014405139721930027\n",
            "Epoch 456/1000, Loss: 0.014387219212949276\n",
            "Epoch 457/1000, Loss: 0.006670542526990175\n",
            "Epoch 458/1000, Loss: 0.006660544313490391\n",
            "Epoch 459/1000, Loss: 0.012817262671887875\n",
            "Epoch 460/1000, Loss: 0.0136471688747406\n",
            "Epoch 461/1000, Loss: 0.011534323915839195\n",
            "Epoch 462/1000, Loss: 0.011130665428936481\n",
            "Epoch 463/1000, Loss: 0.010181278921663761\n",
            "Epoch 464/1000, Loss: 0.011950873769819736\n",
            "Epoch 465/1000, Loss: 0.010660729371011257\n",
            "Epoch 466/1000, Loss: 0.015809904783964157\n",
            "Epoch 467/1000, Loss: 0.011467584408819675\n",
            "Epoch 468/1000, Loss: 0.009755769744515419\n",
            "Epoch 469/1000, Loss: 0.013418436981737614\n",
            "Epoch 470/1000, Loss: 0.010287204757332802\n",
            "Epoch 471/1000, Loss: 0.010317475534975529\n",
            "Epoch 472/1000, Loss: 0.009715072810649872\n",
            "Epoch 473/1000, Loss: 0.007768047507852316\n",
            "Epoch 474/1000, Loss: 0.009958673268556595\n",
            "Epoch 475/1000, Loss: 0.010749473236501217\n",
            "Epoch 476/1000, Loss: 0.010073136538267136\n",
            "Epoch 477/1000, Loss: 0.013466611504554749\n",
            "Epoch 478/1000, Loss: 0.011351428925991058\n",
            "Epoch 479/1000, Loss: 0.011424418538808823\n",
            "Epoch 480/1000, Loss: 0.010837954469025135\n",
            "Epoch 481/1000, Loss: 0.0061556072905659676\n",
            "Epoch 482/1000, Loss: 0.00613485649228096\n",
            "Epoch 483/1000, Loss: 0.008851272985339165\n",
            "Epoch 484/1000, Loss: 0.006205164827406406\n",
            "Epoch 485/1000, Loss: 0.006256408989429474\n",
            "Epoch 486/1000, Loss: 0.011090566404163837\n",
            "Epoch 487/1000, Loss: 0.013011274859309196\n",
            "Epoch 488/1000, Loss: 0.011208162643015385\n",
            "Epoch 489/1000, Loss: 0.011880174279212952\n",
            "Epoch 490/1000, Loss: 0.007110876031219959\n",
            "Epoch 491/1000, Loss: 0.009703923016786575\n",
            "Epoch 492/1000, Loss: 0.007800330873578787\n",
            "Epoch 493/1000, Loss: 0.00613864092156291\n",
            "Epoch 494/1000, Loss: 0.009212932549417019\n",
            "Epoch 495/1000, Loss: 0.010673061944544315\n",
            "Epoch 496/1000, Loss: 0.0059247505851089954\n",
            "Epoch 497/1000, Loss: 0.005952026695013046\n",
            "Epoch 498/1000, Loss: 0.011043401435017586\n",
            "Epoch 499/1000, Loss: 0.005824626889079809\n",
            "Epoch 500/1000, Loss: 0.009898075833916664\n",
            "Epoch 501/1000, Loss: 0.008365795016288757\n",
            "Epoch 502/1000, Loss: 0.009441726841032505\n",
            "Epoch 503/1000, Loss: 0.009649960324168205\n",
            "Epoch 504/1000, Loss: 0.013702213764190674\n",
            "Epoch 505/1000, Loss: 0.009744115173816681\n",
            "Epoch 506/1000, Loss: 0.00962524302303791\n",
            "Epoch 507/1000, Loss: 0.007843449711799622\n",
            "Epoch 508/1000, Loss: 0.009877187199890614\n",
            "Epoch 509/1000, Loss: 0.012045896612107754\n",
            "Epoch 510/1000, Loss: 0.012227554805576801\n",
            "Epoch 511/1000, Loss: 0.009472391568124294\n",
            "Epoch 512/1000, Loss: 0.010652324184775352\n",
            "Epoch 513/1000, Loss: 0.008933469653129578\n",
            "Epoch 514/1000, Loss: 0.01283278502523899\n",
            "Epoch 515/1000, Loss: 0.009002660401165485\n",
            "Epoch 516/1000, Loss: 0.008969100192189217\n",
            "Epoch 517/1000, Loss: 0.011637257412075996\n",
            "Epoch 518/1000, Loss: 0.010567188262939453\n",
            "Epoch 519/1000, Loss: 0.005562691017985344\n",
            "Epoch 520/1000, Loss: 0.005419791676104069\n",
            "Epoch 521/1000, Loss: 0.0104978634044528\n",
            "Epoch 522/1000, Loss: 0.007243827451020479\n",
            "Epoch 523/1000, Loss: 0.0070573436096310616\n",
            "Epoch 524/1000, Loss: 0.009024444967508316\n",
            "Epoch 525/1000, Loss: 0.00567896943539381\n",
            "Epoch 526/1000, Loss: 0.005310002714395523\n",
            "Epoch 527/1000, Loss: 0.005275276023894548\n",
            "Epoch 528/1000, Loss: 0.012263794429600239\n",
            "Epoch 529/1000, Loss: 0.008535386994481087\n",
            "Epoch 530/1000, Loss: 0.00525343744084239\n",
            "Epoch 531/1000, Loss: 0.010356266051530838\n",
            "Epoch 532/1000, Loss: 0.013714604079723358\n",
            "Epoch 533/1000, Loss: 0.006101286504417658\n",
            "Epoch 534/1000, Loss: 0.010004820302128792\n",
            "Epoch 535/1000, Loss: 0.006440743338316679\n",
            "Epoch 536/1000, Loss: 0.007070690859109163\n",
            "Epoch 537/1000, Loss: 0.006677935365587473\n",
            "Epoch 538/1000, Loss: 0.010689250193536282\n",
            "Epoch 539/1000, Loss: 0.005175161641091108\n",
            "Epoch 540/1000, Loss: 0.0054003470577299595\n",
            "Epoch 541/1000, Loss: 0.005626952741295099\n",
            "Epoch 542/1000, Loss: 0.008170876652002335\n",
            "Epoch 543/1000, Loss: 0.012462968938052654\n",
            "Epoch 544/1000, Loss: 0.010033409111201763\n",
            "Epoch 545/1000, Loss: 0.00901490356773138\n",
            "Epoch 546/1000, Loss: 0.008512256667017937\n",
            "Epoch 547/1000, Loss: 0.005290533881634474\n",
            "Epoch 548/1000, Loss: 0.010172754526138306\n",
            "Epoch 549/1000, Loss: 0.0048902896232903\n",
            "Epoch 550/1000, Loss: 0.004977480974048376\n",
            "Epoch 551/1000, Loss: 0.007430251222103834\n",
            "Epoch 552/1000, Loss: 0.009857861325144768\n",
            "Epoch 553/1000, Loss: 0.00941870454698801\n",
            "Epoch 554/1000, Loss: 0.0049127754755318165\n",
            "Epoch 555/1000, Loss: 0.004923943430185318\n",
            "Epoch 556/1000, Loss: 0.007419411558657885\n",
            "Epoch 557/1000, Loss: 0.004892365075647831\n",
            "Epoch 558/1000, Loss: 0.0069081736728549\n",
            "Epoch 559/1000, Loss: 0.0074387481436133385\n",
            "Epoch 560/1000, Loss: 0.005072455853223801\n",
            "Epoch 561/1000, Loss: 0.0052422708831727505\n",
            "Epoch 562/1000, Loss: 3.3013741970062256\n",
            "Epoch 563/1000, Loss: 0.00517375348135829\n",
            "Epoch 564/1000, Loss: 0.009960870258510113\n",
            "Epoch 565/1000, Loss: 0.005408504046499729\n",
            "Epoch 566/1000, Loss: 0.005719927605241537\n",
            "Epoch 567/1000, Loss: 0.006585472263395786\n",
            "Epoch 568/1000, Loss: 0.006144669838249683\n",
            "Epoch 569/1000, Loss: 0.006154775153845549\n",
            "Epoch 570/1000, Loss: 0.0061358376406133175\n",
            "Epoch 571/1000, Loss: 0.00606749439612031\n",
            "Epoch 572/1000, Loss: 0.005970517639070749\n",
            "Epoch 573/1000, Loss: 0.006635837722569704\n",
            "Epoch 574/1000, Loss: 0.013308441266417503\n",
            "Epoch 575/1000, Loss: 0.005679722409695387\n",
            "Epoch 576/1000, Loss: 0.005658192094415426\n",
            "Epoch 577/1000, Loss: 0.011432678438723087\n",
            "Epoch 578/1000, Loss: 0.010366206057369709\n",
            "Epoch 579/1000, Loss: 0.005782555788755417\n",
            "Epoch 580/1000, Loss: 0.011508507654070854\n",
            "Epoch 581/1000, Loss: 0.009539978578686714\n",
            "Epoch 582/1000, Loss: 0.006847732700407505\n",
            "Epoch 583/1000, Loss: 0.010635643266141415\n",
            "Epoch 584/1000, Loss: 0.009497699327766895\n",
            "Epoch 585/1000, Loss: 0.01496834121644497\n",
            "Epoch 586/1000, Loss: 0.012727074325084686\n",
            "Epoch 587/1000, Loss: 0.005333355162292719\n",
            "Epoch 588/1000, Loss: 0.010913844220340252\n",
            "Epoch 589/1000, Loss: 0.004995493683964014\n",
            "Epoch 590/1000, Loss: 0.004930953495204449\n",
            "Epoch 591/1000, Loss: 0.008668330498039722\n",
            "Epoch 592/1000, Loss: 0.004956877790391445\n",
            "Epoch 593/1000, Loss: 0.010534866712987423\n",
            "Epoch 594/1000, Loss: 0.004906673915684223\n",
            "Epoch 595/1000, Loss: 0.010191792622208595\n",
            "Epoch 596/1000, Loss: 0.007709133438766003\n",
            "Epoch 597/1000, Loss: 0.0047905705869197845\n",
            "Epoch 598/1000, Loss: 0.006455051712691784\n",
            "Epoch 599/1000, Loss: 0.004673488903790712\n",
            "Epoch 600/1000, Loss: 0.004707641433924437\n",
            "Epoch 601/1000, Loss: 0.005289108492434025\n",
            "Epoch 602/1000, Loss: 0.01007738895714283\n",
            "Epoch 603/1000, Loss: 0.004593633580952883\n",
            "Epoch 604/1000, Loss: 0.008401412516832352\n",
            "Epoch 605/1000, Loss: 0.004510186146944761\n",
            "Epoch 606/1000, Loss: 0.00878707505762577\n",
            "Epoch 607/1000, Loss: 0.004487111233174801\n",
            "Epoch 608/1000, Loss: 0.004476582631468773\n",
            "Epoch 609/1000, Loss: 0.009609424509108067\n",
            "Epoch 610/1000, Loss: 0.00916576012969017\n",
            "Epoch 611/1000, Loss: 0.009201047010719776\n",
            "Epoch 612/1000, Loss: 0.004532028455287218\n",
            "Epoch 613/1000, Loss: 0.007098659873008728\n",
            "Epoch 614/1000, Loss: 0.0061902194283902645\n",
            "Epoch 615/1000, Loss: 0.013055773451924324\n",
            "Epoch 616/1000, Loss: 0.004303600173443556\n",
            "Epoch 617/1000, Loss: 0.004338591359555721\n",
            "Epoch 618/1000, Loss: 0.004388076718896627\n",
            "Epoch 619/1000, Loss: 0.008045642636716366\n",
            "Epoch 620/1000, Loss: 0.0069515821523964405\n",
            "Epoch 621/1000, Loss: 0.004270585719496012\n",
            "Epoch 622/1000, Loss: 0.009526913054287434\n",
            "Epoch 623/1000, Loss: 0.0059098368510603905\n",
            "Epoch 624/1000, Loss: 0.004747346509248018\n",
            "Epoch 625/1000, Loss: 0.00760843837633729\n",
            "Epoch 626/1000, Loss: 0.009805034846067429\n",
            "Epoch 627/1000, Loss: 0.008767303079366684\n",
            "Epoch 628/1000, Loss: 0.004219260066747665\n",
            "Epoch 629/1000, Loss: 0.006973606534302235\n",
            "Epoch 630/1000, Loss: 0.005473034456372261\n",
            "Epoch 631/1000, Loss: 0.00633475324138999\n",
            "Epoch 632/1000, Loss: 0.008601140230894089\n",
            "Epoch 633/1000, Loss: 0.007161642890423536\n",
            "Epoch 634/1000, Loss: 0.005012167152017355\n",
            "Epoch 635/1000, Loss: 0.009756147861480713\n",
            "Epoch 636/1000, Loss: 0.004025198053568602\n",
            "Epoch 637/1000, Loss: 0.011473674327135086\n",
            "Epoch 638/1000, Loss: 0.008845195174217224\n",
            "Epoch 639/1000, Loss: 0.0065405103377997875\n",
            "Epoch 640/1000, Loss: 0.007658971007913351\n",
            "Epoch 641/1000, Loss: 0.006777829024940729\n",
            "Epoch 642/1000, Loss: 0.007002682890743017\n",
            "Epoch 643/1000, Loss: 0.005445759277790785\n",
            "Epoch 644/1000, Loss: 0.005012906156480312\n",
            "Epoch 645/1000, Loss: 0.008413144387304783\n",
            "Epoch 646/1000, Loss: 0.005146382376551628\n",
            "Epoch 647/1000, Loss: 0.0038729484658688307\n"
          ]
        }
      ],
      "source": [
        "# Fine-tune the model\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "num_epochs = 1000\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in dataloader:\n",
        "        input_ids, label_ids = batch\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(input_ids=input_ids, labels=label_ids)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model.save_pretrained(\"fine_tuned_model\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "EFJ9CJE59M94"
      },
      "outputs": [],
      "source": [
        "# Inference\n",
        "def predict_masked_letter(word):\n",
        "    tokenized_word = tokenizer(word, return_tensors=\"pt\", is_split_into_words=True)\n",
        "    mask_index = torch.where(tokenized_word[\"input_ids\"][0] == tokenizer.mask_token_id)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**tokenized_word)\n",
        "        print(outputs)\n",
        "        predictions = torch.argmax(outputs.logits[0, mask_index]).item()\n",
        "        predicted_letter = tokenizer.convert_ids_to_tokens(predictions)\n",
        "\n",
        "    return predicted_letter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "TiKlfT95E9Bd"
      },
      "outputs": [],
      "source": [
        "#abaptiston\n",
        "word = f\"[MASK]onsoon\"\n",
        "tokenized_word = tokenizer(word, return_tensors=\"pt\")\n",
        "# Find the index of the masked token\n",
        "mask_index = torch.where(tokenized_word[\"input_ids\"][0] == tokenizer.mask_token_id)[0].item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "4a6D9hOpLT-E"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['a', 'r', 'o', 's', 'n', 'l', 'i', 'u', 'b', 'c', 'd', 'e', 't', 'k', 'h', 'm', 'g', 'p', 'v', 'f']\n",
            "['a', 'r', 'o', 's', 'n', 'l', 'i', 'u', 'b', 'c', 'd', 'e', 't', 'k', 'h', 'm', 'g', 'p', 'v', 'f']\n",
            "['a', 'r', 'o', 's', 'n', 'l', 'i', 'u', 'b', 'c', 'd', 'e', 't', 'k', 'h', 'm', 'g', 'p', 'v', 'f']\n",
            "['a', 'r', 'l', 'i', 'u', 'b', 'c', 'd', 'e', 't', 'k', 'h', 'm', 'g', 'p', 'v', 'f']\n"
          ]
        }
      ],
      "source": [
        "# Get the model's output\n",
        "with torch.no_grad():\n",
        "    outputs = model(**tokenized_word)\n",
        "\n",
        "# Extract logits for the masked token\n",
        "mask_token_logits = outputs.logits[0, mask_index]\n",
        "\n",
        "# Get the top 10 predictions\n",
        "top_20_predictions = torch.topk(mask_token_logits, 20).indices\n",
        "\n",
        "# Convert the token IDs to tokens (letters)\n",
        "top_20_tokens = [tokenizer.convert_ids_to_tokens(pred.item()) for pred in top_20_predictions]\n",
        "print(top_20_tokens)\n",
        "top_20_tokens = [pred.replace(\"#\",\"\") for pred in top_20_tokens]\n",
        "print(top_20_tokens)\n",
        "top_20_tokens=list(dict.fromkeys(top_20_tokens))\n",
        "print(top_20_tokens)\n",
        "letters = \" \".join(list(word)).split(\" \")\n",
        "top_words = [let for let in top_20_tokens if let not in letters]\n",
        "print(top_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwhNfXOfDokY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
